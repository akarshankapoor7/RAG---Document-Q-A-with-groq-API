This project is a Retrieval-Augmented Generation (RAG) Document Q&A application built with Streamlit, LangChain, and Groq’s Llama3 model. It allows users to query information from PDF documents stored in a specified directory (dox). The system processes PDFs using PyPDFDirectoryLoader, splits them into chunks with RecursiveCharacterTextSplitter, and embeds them into a FAISS vector store using HuggingFace’s all-MiniLM-L6-v2 embeddings. When a user inputs a query, the app retrieves relevant document chunks via a retrieval chain and generates answers using the Llama3-8b-8192 model, displaying both the response and similar document excerpts. The interface features a button to initialize the vector database and a text input for queries, with response time tracking. Environment variables (e.g., API keys) are managed via a .env file. The requirements.txt includes dependencies like streamlit, langchain, faiss-cpu, and sentence-transformers. Designed for research or document analysis, it ensures answers are contextually grounded in the provided PDFs, leveraging RAG to enhance accuracy and relevance.
